# Home-defaul-risk-competition
This .ipynb file is the my solution for Kaggle Home Default Risk Competition
Cleaning
Project report
I started with learning the datasets and understanding the features. I created histograms for each variable in datasets and found that the column - DAYS_EMPLOYED at the “application” dataset has outliers, because all numbers related with time in this dataset and other dataset have negative values, but, in this column, there were large positive numbers which were possibly used for code some people with specific problem or maybe people who didn’t provide information. Furthermore, the same issue was found at the “previous_application” dataset at the DAYS_FIRST_DRAWING, DAYS_FIRST_DUE, DAYS_LAST_DUE_1ST_VERSION, DAYS_LAST_DUE, DAYS_TERMINATION columns.
Moreover, on the stage of applying logistic regression, I dropped columns with a lot of missing values (treshold 80%) and collinear columns. In addition, infinity values that can emerge are replaced by np.na before model is applied if it is LGB model, otherwise, in case of logistic regression, imputation is performed with adding new column to indicate missing values.
Feature engineering
The first features I added were features from domain of credits. These are features with nonlinear interaction: features related with an income and credit amount (payment rate - become the most important feature in LGBM, income over credit etc), time features (e.g. DAYS_EMPLOYED / DAYS_BIRTH - days worked by 1 day of life), crossed features (e.g. AMT_INCOME_TOTAL/ DAYS_BIRTH - income by each day of life). Next, I created polynomial features with degree 3. Some of polynomial feature become very important for example: EXT_SOURCE_1 * EXT_SOURCE_2 * EXT_SOURCE_3, EXT_SOURCE_1 ^ 2 * DAYS_BIRTH.
For other tables, I wrote aggregation functions for categorical and numerical variables. Other data in dataset has deep connections for example: bureau_balance connected to bureau which is connected to application. Thus, I can do few levels of aggregation. In case of bureau, I first calculated sum, and mean for each status in bureau_balance and then merged with bureau and applied the second time aggregation (now numeric and categorical) by bureau ID. The same deep aggregation I applied to other tables. Numeric data aggregations includes count, mean, sum, std, min, max statistics.
In addition, the more advanced features were created. I decided to take into account not only frequencies of statuses but the longest runs of statuses (for example: CCCCXXCXXX - C=4, X=3). Moreover, the pairs of statuses in sequence may be important, so I calculated frequencies of CX, C0, XC, X1 etc. Since there are many such information, for each loan ID, I calculated regular statistics: mean, median, skew, std etc. I got around 500 features from statuses which gives us increase in around 0.003 without dimension reduction.
Next, I found a discussion when was described other features which were made by grouping active loans by month. I decided to write a class doing this kind of grouping in order to use it for different data. Then, it was done two grouping one by month and one by overdue days. For each of 96 months I create statistics for each columns in ‘bureau’ and I got around 8000 features. In case of overdue days, I made groups of overdue by 10 days (installments) for each column and calculated regular statistics for each loan.
I faced with the problem that I couldn’t run model with so many features even on Google Colab with 32 GB RAM available. I tried PCA reduction on statuses features and it turned out that after PCA, features gave not so much increase in score. Thus, I decided to use LDA which gave us just few features for every group of advanced features. In spite of that cross validation brought us 0.8 with LDA features, submission score was lower than I already had. I concluded that since LDA is supervised method it puts information of train labels in features, thus model might overfit. I couldn’t run model with new features because of lack of computational power, thus I stayed with the score I already had.
Training
I started from logistic regression which gave us 0.67 score on application and bureau data after reducing dimension and applying Box-Cox transformation. Since logistic regression works poorly with missing data and multicoliniarity, I decided to move to decision trees algorithms. I tested two algorithms based on decision trees: CatBost (algorithm which was created by Yandex) and Light GBM (LGBM). The performance of LGBM was slightly better 0.771 against 0.768 and these two results were much better than performance of logistic regression.
In our project, the training via LGBM is performed in this way: 1) first I transform categorical data via ‘one-hot’ encoding; 2) the training data split into K chunks with shuffled data - K-1 blocks (joined together) as train and 1 block as validation (always different) 3) train data with labels and validation data feed to LGB classifier 4) after each iteration predictions (and feature importance) are calculated for current parameters of classifier 5) after iteration the average of predictions score are calculated to get the final list of probabilities.
In addition, I found optimized parameters, particularly for that home credit problem, for LGBM. Optimized parameters gave around 0.004 increase in score.
